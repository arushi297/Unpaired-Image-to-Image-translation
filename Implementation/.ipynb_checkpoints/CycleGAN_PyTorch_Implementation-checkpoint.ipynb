{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imNXxFGjhhqF"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DdQ79H2WJ_9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import argparse\n",
    "import os, itertools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqwVGEA1hftH"
   },
   "source": [
    "# Dataset preparation and Parameter Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xFhqW0sZDvU"
   },
   "source": [
    "We use the argparse module to define and parse command line arguments. It sets parameters for the data set, model, and learning. It also defines directories for loading data and saving results. The values of the arguments are printed and stored in the params variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCcbagfqak3Z",
    "outputId": "e719d2f6-cc69-435a-da89-e50c6b70d507"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#Data Set Parameter\n",
    "parser.add_argument('--dataset', required=False, default='summer2winter', help='input dataset')\n",
    "parser.add_argument('--batch_size', type=int, default=1, help='train batch size')\n",
    "parser.add_argument('--input_size', type=int, default=256, help='input size')\n",
    "parser.add_argument('--resize_scale', type=int, default=286, help='resize scale (0 is false)')\n",
    "parser.add_argument('--crop_size', type=int, default=256, help='crop size (0 is false)')\n",
    "parser.add_argument('--fliplr', type=bool, default=True, help='random fliplr True of False')\n",
    "\n",
    "#Model Parameters \n",
    "parser.add_argument('--ngf', type=int, default=32) # number of generator filters\n",
    "parser.add_argument('--ndf', type=int, default=64) # number of discriminator filters\n",
    "parser.add_argument('--num_resnet', type=int, default=6, help='number of resnet blocks in generator')\n",
    "\n",
    "#Learning Parameters\n",
    "parser.add_argument('--num_epochs', type=int, default=70, help='number of train epochs')\n",
    "parser.add_argument('--decay_epoch', type=int, default=100, help='start decaying learning rate after this number')\n",
    "parser.add_argument('--lrG', type=float, default=0.0002, help='learning rate for generator, default=0.0002')\n",
    "parser.add_argument('--lrD', type=float, default=0.0002, help='learning rate for discriminator, default=0.0002')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "parser.add_argument('--lambdaA', type=float, default=10, help='lambdaA for cycle loss')\n",
    "parser.add_argument('--lambdaB', type=float, default=10, help='lambdaB for cycle loss')\n",
    "params = parser.parse_args([])\n",
    "print(params)\n",
    "\n",
    "# Directories for loading data and saving results\n",
    "data_dir = '/content/data'\n",
    "save_dir = '/content/results/'\n",
    "plot_gif_dir = '/content/results/plot_gif/'\n",
    "test_res_dir = '/content/test_results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFEh_bw4ZONG"
   },
   "source": [
    "Ensuring that the directories exist and can be used for storing and loading data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIi5kxbTl8tQ"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir) \n",
    "\n",
    "if not os.path.exists(test_res_dir):\n",
    "    os.makedirs(test_res_dir)   \n",
    "\n",
    "if not os.path.exists(plot_gif_dir):\n",
    "    os.makedirs(plot_gif_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35uMv2lMWdER",
    "outputId": "39e90b90-d033-4612-a857-e65d43d6cfc9"
   },
   "outputs": [],
   "source": [
    "os.chdir(data_dir)\n",
    "!pip install kaggle --upgrade\n",
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "!kaggle datasets download -d balraj98/summer2winter-yosemite\n",
    "!unzip summer2winter-yosemite.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2dmc5KciQGg"
   },
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ8OGte0ZeTg"
   },
   "source": [
    "Here we are defining a data augmentation pipeline using transforms.Compose(). The pipeline resizes the input image to (params.input_size, params.input_size), converts it to a tensor, and normalizes the pixel values to have a mean of (0.5, 0.5, 0.5) and a standard deviation of (0.5, 0.5, 0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA0d9JL-amXE"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((params.input_size,params.input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfuw98WkiSL5"
   },
   "source": [
    "# Defining Auxiliary Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m66VOOVUamyL"
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch dataset class for loading image data from a folder.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): Path to the folder containing image files.\n",
    "        subfolder (str, optional): Name of the subfolder within image_dir to use. Default is 'train'.\n",
    "        transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version.\n",
    "        resize_scale (int, optional): Size to resize the image to. Default is None (no resizing).\n",
    "        crop_size (int, optional): Size to crop the image to. Default is None (no cropping).\n",
    "        fliplr (bool, optional): Whether or not to randomly flip the image horizontally. Default is False (no flipping).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, subfolder='train', transform=None, resize_scale=None, crop_size=None, fliplr=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            image_dir (str): Path to the folder containing image files.\n",
    "            subfolder (str, optional): Name of the subfolder within image_dir to use. Default is 'train'.\n",
    "            transform (callable, optional): A function/transform that takes in a PIL image and returns a transformed version.\n",
    "            resize_scale (int, optional): Size to resize the image to. Default is None (no resizing).\n",
    "            crop_size (int, optional): Size to crop the image to. Default is None (no cropping).\n",
    "            fliplr (bool, optional): Whether or not to randomly flip the image horizontally. Default is False (no flipping).\n",
    "        \"\"\"\n",
    "\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.input_path = os.path.join(image_dir, subfolder)\n",
    "        self.image_filenames = [x for x in sorted(os.listdir(self.input_path))]\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.resize_scale = resize_scale\n",
    "        self.crop_size = crop_size\n",
    "        self.fliplr = fliplr\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Load and preprocess an image from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the image to load.\n",
    "\n",
    "        Returns:\n",
    "            img (PIL Image): The loaded and preprocessed image.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load Image\n",
    "        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n",
    "        img = Image.open(img_fn).convert('RGB')\n",
    "\n",
    "        # preprocessing\n",
    "        if self.resize_scale:\n",
    "            img = img.resize((self.resize_scale, self.resize_scale), Image.BILINEAR)\n",
    "\n",
    "        if self.crop_size:\n",
    "            x = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            y = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            img = img.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "        if self.fliplr:\n",
    "            if random.random() < 0.5:\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            length (int): The number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFCEB2erZ27U"
   },
   "source": [
    "We initialize two DatasetFromFolder objects train_data_A and train_data_B which read the image files from the directories trainA and trainB respectively. The images are then preprocessed using the transform pipeline defined earlier, with additional options for resizing, cropping and horizontal flipping. The resulting preprocessed images are then loaded into DataLoader objects train_data_loader_A and train_data_loader_B respectively, which will be used for iterating over the training data during the training process. The batch_size parameter determines how many images are loaded into memory at once, and the shuffle parameter shuffles the order of the images to ensure the model sees a different order of images during each epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLjeM3Dsam1S"
   },
   "outputs": [],
   "source": [
    "train_data_A = DatasetFromFolder(data_dir, subfolder='trainA', transform=transform,\n",
    "                                resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "train_data_B = DatasetFromFolder(data_dir, subfolder='trainB', transform=transform,\n",
    "                                resize_scale=params.resize_scale, crop_size=params.crop_size, fliplr=params.fliplr)\n",
    "\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B, batch_size=params.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cnW0QbzaKlN"
   },
   "source": [
    "Here we define the data loaders for the test dataset, test_data_A_loader and test_data_B_loader, which are used to load images for testing the trained model. The specific test images, test_real_A_data and test_real_B_data, are obtained by calling the __getitem__ method on the train data loaders for train_data_A and train_data_B respectively, and then unsqueezing them to create 4D tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-myhhBJcEAe",
    "outputId": "7e142fa4-3c27-4726-d2e8-cffeaf780b93"
   },
   "outputs": [],
   "source": [
    "test_data_A = DatasetFromFolder(data_dir, subfolder='testA', transform=transform)\n",
    "\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "test_data_B = DatasetFromFolder(data_dir, subfolder='testB', transform=transform)\n",
    "\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Get specific test images\n",
    "test_real_A_data = train_data_A.__getitem__(11).unsqueeze(0) # Convert to 4d tensor (BxNxHxW)\n",
    "test_real_B_data = train_data_B.__getitem__(91).unsqueeze(0)\n",
    "print(test_real_A_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zkpcmUDiY4V"
   },
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWNfFWvFY1I4"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional block that consists of a convolutional layer and an optional batch normalization layer\n",
    "    followed by an activation function.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input channels.\n",
    "        output_size (int): The number of output channels.\n",
    "        kernel_size (int, optional): The size of the kernel. Default is 3.\n",
    "        stride (int, optional): The stride of the convolution. Default is 2.\n",
    "        padding (int, optional): The padding added to the input. Default is 1.\n",
    "        activation (str, optional): The activation function to be applied. Can be 'relu', 'lrelu', 'tanh', or 'no_act'.\n",
    "            Default is 'relu'.\n",
    "        batch_norm (bool, optional): Whether to apply batch normalization. Default is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, kernel_size=3, stride=2, padding=1, activation='relu', batch_norm=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_size, output_size, kernel_size, stride, padding)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.InstanceNorm2d(output_size)\n",
    "        self.activation = activation\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.lrelu = torch.nn.LeakyReLU(0.2, True)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies the convolutional block to the input.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying the convolutional block.\n",
    "        \"\"\"\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.conv(x))\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(out)\n",
    "        elif self.activation == 'lrelu':\n",
    "            return self.lrelu(out)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh(out)\n",
    "        elif self.activation == 'no_act':\n",
    "            return out\n",
    "\n",
    "\n",
    "class DeconvBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A deconvolutional block that consists of a deconvolutional layer and an optional batch normalization layer\n",
    "    followed by an activation function.\n",
    "\n",
    "    Args:\n",
    "        input_size (int): The number of input channels.\n",
    "        output_size (int): The number of output channels.\n",
    "        kernel_size (int, optional): The size of the kernel. Default is 3.\n",
    "        stride (int, optional): The stride of the deconvolution. Default is 2.\n",
    "        padding (int, optional): The padding added to the input. Default is 1.\n",
    "        output_padding (int, optional): The additional size added to one side of the output shape. Default is 1.\n",
    "        activation (str, optional): The activation function to be applied. Default is 'relu'.\n",
    "        batch_norm (bool, optional): Whether to apply batch normalization. Default is True.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, kernel_size=3, stride=2, padding=1, output_padding=1, activation='relu', batch_norm=True):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(input_size, output_size, kernel_size, stride, padding, output_padding)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.InstanceNorm2d(output_size)\n",
    "        self.activation = activation\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "    \"\"\"\n",
    "    Performs a forward pass of the DeconvBlock.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The output tensor.\n",
    "    \"\"\"\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.deconv(x))\n",
    "        else:\n",
    "            out = self.deconv(x)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(out)\n",
    "        elif self.activation == 'lrelu':\n",
    "            return self.lrelu(out)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh(out)\n",
    "        elif self.activation == 'no_act':\n",
    "            return out\n",
    "\n",
    "\n",
    "class ResnetBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that consists of two convolutional layers, each followed by batch normalization and ReLU activation,\n",
    "    and an additional reflection padding layer. It adds the input tensor to the output tensor of the block to create a\n",
    "    residual connection.\n",
    "    \n",
    "    Args:\n",
    "        num_filter (int): The number of filters in the convolutional layers.\n",
    "        kernel_size (int, optional): The size of the kernel in the convolutional layers. Default is 3.\n",
    "        stride (int, optional): The stride of the convolutional layers. Default is 1.\n",
    "        padding (int, optional): The padding added to the input by the reflection padding layer. Default is 0.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_filter, kernel_size=3, stride=1, padding=0):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        conv1 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride, padding)\n",
    "        conv2 = torch.nn.Conv2d(num_filter, num_filter, kernel_size, stride, padding)\n",
    "        bn = torch.nn.InstanceNorm2d(num_filter)\n",
    "        relu = torch.nn.ReLU(True)\n",
    "        pad = torch.nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.resnet_block = torch.nn.Sequential(\n",
    "            pad,\n",
    "            conv1,\n",
    "            bn,\n",
    "            relu,\n",
    "            pad,\n",
    "            conv2,\n",
    "            bn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply forward pass of the residual block on the input tensor x and return the output tensor.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, num_filter, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): Output tensor of shape (batch_size, num_filter, height, width).\n",
    "        \"\"\"\n",
    "        out = self.resnet_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Generator(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A generator neural network model for image-to-image translation tasks.\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): The number of channels in the input image.\n",
    "        num_filter (int): The number of filters in the first convolutional layer of the encoder.\n",
    "        output_dim (int): The number of channels in the output image.\n",
    "        num_resnet (int): The number of residual blocks in the generator.\n",
    "\n",
    "    Attributes:\n",
    "        pad (torch.nn.ReflectionPad2d): The reflection padding layer.\n",
    "        conv1 (ConvBlock): The first convolutional block of the encoder.\n",
    "        conv2 (ConvBlock): The second convolutional block of the encoder.\n",
    "        conv3 (ConvBlock): The third convolutional block of the encoder.\n",
    "        resnet_blocks (torch.nn.Sequential): The sequence of residual blocks in the generator.\n",
    "        deconv1 (DeconvBlock): The first deconvolutional block of the decoder.\n",
    "        deconv2 (DeconvBlock): The second deconvolutional block of the decoder.\n",
    "        deconv3 (ConvBlock): The third convolutional block of the decoder.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Performs a forward pass through the generator.\n",
    "        normal_weight_init(mean, std): Initializes the weights of the generator with normally distributed random values.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_filter, output_dim, num_resnet):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Reflection padding\n",
    "        self.pad = torch.nn.ReflectionPad2d(3)\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(input_dim, num_filter, kernel_size=7, stride=1, padding=0)\n",
    "        self.conv2 = ConvBlock(num_filter, num_filter * 2)\n",
    "        self.conv3 = ConvBlock(num_filter * 2, num_filter * 4)\n",
    "        # Resnet blocks\n",
    "        self.resnet_blocks = []\n",
    "        for i in range(num_resnet):\n",
    "            self.resnet_blocks.append(ResnetBlock(num_filter * 4))\n",
    "        self.resnet_blocks = torch.nn.Sequential(*self.resnet_blocks)\n",
    "        # Decoder\n",
    "        self.deconv1 = DeconvBlock(num_filter * 4, num_filter * 2)\n",
    "        self.deconv2 = DeconvBlock(num_filter * 2, num_filter)\n",
    "        self.deconv3 = ConvBlock(num_filter, output_dim, kernel_size=7, stride=1, padding=0, activation='tanh', batch_norm=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the generator.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input image tensor.\n",
    "\n",
    "        Returns:\n",
    "            The output image tensor generated by the generator.\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        enc1 = self.conv1(self.pad(x))\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        # Resnet blocks\n",
    "        res = self.resnet_blocks(enc3)\n",
    "        # Decoder\n",
    "        dec1 = self.deconv1(res)\n",
    "        dec2 = self.deconv2(dec1)\n",
    "        out = self.deconv3(self.pad(dec2))\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the generator with normally distributed random values.\n",
    "\n",
    "        Args:\n",
    "            mean (float): The mean of the normal distribution.\n",
    "            std (float): The standard deviation of the normal distribution.\n",
    "        \"\"\"\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)\n",
    "            if isinstance(m, DeconvBlock):\n",
    "                torch.nn.init.normal(m.deconv.weight, mean, std)\n",
    "            if isinstance(m, ResnetBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)\n",
    "                torch.nn.init.constant(m.conv.bias, 0)\n",
    "\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional neural network that is used as a discriminator in a Generative Adversarial Network (GAN).\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): the number of input channels for the first convolutional layer.\n",
    "        num_filter (int): the number of filters in the first convolutional layer.\n",
    "        output_dim (int): the number of output channels for the last convolutional layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_filter, output_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        conv1 = ConvBlock(input_dim, num_filter, kernel_size=4, stride=2, padding=1, activation='lrelu', batch_norm=False)\n",
    "        conv2 = ConvBlock(num_filter, num_filter * 2, kernel_size=4, stride=2, padding=1, activation='lrelu')\n",
    "        conv3 = ConvBlock(num_filter * 2, num_filter * 4, kernel_size=4, stride=2, padding=1, activation='lrelu')\n",
    "        conv4 = ConvBlock(num_filter * 4, num_filter * 8, kernel_size=4, stride=1, padding=1, activation='lrelu')\n",
    "        conv5 = ConvBlock(num_filter * 8, output_dim, kernel_size=4, stride=1, padding=1, activation='no_act', batch_norm=False)\n",
    "\n",
    "        self.conv_blocks = torch.nn.Sequential(\n",
    "            conv1,\n",
    "            conv2,\n",
    "            conv3,\n",
    "            conv4,\n",
    "            conv5\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feeds the input tensor through the discriminator network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): the input tensor.\n",
    "\n",
    "        Returns:\n",
    "            The output tensor after it has passed through the discriminator network.\n",
    "        \"\"\"\n",
    "        out = self.conv_blocks(x)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self, mean=0.0, std=0.02):\n",
    "        \"\"\"\n",
    "        Initializes the weights of the convolutional layers using a normal distribution with the given mean and standard deviation.\n",
    "\n",
    "        Args:\n",
    "            mean (float): the mean of the normal distribution (default=0.0).\n",
    "            std (float): the standard deviation of the normal distribution (default=0.02).\n",
    "        \"\"\"\n",
    "        for m in self.children():\n",
    "            if isinstance(m, ConvBlock):\n",
    "                torch.nn.init.normal(m.conv.weight, mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeMRscCydP4d"
   },
   "source": [
    "The generators G_A and G_B are defined with 3 input channels, 'params.ngf' number of filters in the first layer, 3 output channels, and 'params.num_resnet' number of residual blocks. The discriminators D_A and D_B are defined with 3 input channels, 'params.ndf' number of filters in the first layer, and 1 output channel. The normal_weight_init method is called on each of the generators and discriminators to initialize their weights. Lastly, the models are moved to the GPU by calling the 'cuda()' method on each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2gTLSsicIcZ",
    "outputId": "119e96e7-8ada-4c07-92bf-46c942c58e1d"
   },
   "outputs": [],
   "source": [
    "G_A = Generator(3, params.ngf, 3, params.num_resnet) # input_dim, num_filter, output_dim, num_resnet\n",
    "G_B = Generator(3, params.ngf, 3, params.num_resnet)\n",
    "\n",
    "D_A = Discriminator(3, params.ndf, 1) # input_dim, num_filter, output_dim\n",
    "D_B = Discriminator(3, params.ndf, 1)\n",
    "\n",
    "G_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "print(G_A.cuda())\n",
    "print(G_B.cuda())\n",
    "print(D_A.cuda())\n",
    "print(D_B.cuda())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uutXfLnudai9"
   },
   "source": [
    "Initializing the three optimizers:\n",
    "\n",
    "1. G_optimizer for optimizing the generators G_A and G_B with the Adam optimizer\n",
    "2. D_A_optimizer for optimizing the discriminator D_A with the Adam optimizer\n",
    "3. D_B_optimizer for optimizing the discriminator D_B with the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mzBFZb0cOMg"
   },
   "outputs": [],
   "source": [
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=params.lrG, betas=(params.beta1, params.beta2))\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=params.lrD, betas=(params.beta1, params.beta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWbOvG0liHLg"
   },
   "source": [
    "# Defining Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PohkGgUWcWBn"
   },
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch tensor to a NumPy array on the CPU.\n",
    "\n",
    "    Args:\n",
    "    x: A PyTorch tensor.\n",
    "\n",
    "    Returns:\n",
    "    A NumPy array on the CPU.\n",
    "    \"\"\"\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def to_var(x):\n",
    "    \"\"\"\n",
    "    Converts a tensor to a PyTorch Variable and moves it to the GPU if CUDA is available.\n",
    "\n",
    "    Args:\n",
    "    x: The tensor to be converted.\n",
    "\n",
    "    Returns:\n",
    "    A PyTorch Variable containing the input tensor, moved to the GPU if available.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "\n",
    "# De-normalization\n",
    "def denorm(x):\n",
    "    \"\"\"\n",
    "    De-normalizes the input tensor by scaling it from the range [-1, 1] to [0, 1].\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): Input tensor to be de-normalized.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: De-normalized tensor.\n",
    "    \"\"\"\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "# Plot losses\n",
    "def plot_loss(avg_losses, num_epochs, save=False, save_dir='results/', show=False):\n",
    "    \"\"\"\n",
    "    Plots the losses of a GAN model.\n",
    "\n",
    "    Args:\n",
    "        avg_losses (list): A list of average loss values for each model (D_A, D_B, G_A, G_B, cycle_A, cycle_B) at different epochs.\n",
    "        num_epochs (int): The total number of epochs.\n",
    "        save (bool, optional): If True, saves the plot to a file. Defaults to False.\n",
    "        save_dir (str, optional): The directory where the plot should be saved. Defaults to 'results/'.\n",
    "        show (bool, optional): If True, displays the plot. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, num_epochs)\n",
    "    temp = 0.0\n",
    "    for i in range(len(avg_losses)):\n",
    "        temp = max(np.max(avg_losses[i]), temp)\n",
    "    ax.set_ylim(0, temp*1.1)\n",
    "    plt.xlabel('# of Epochs')\n",
    "    plt.ylabel('Loss values')\n",
    "\n",
    "    plt.plot(avg_losses[0], label='D_A')\n",
    "    plt.plot(avg_losses[1], label='D_B')\n",
    "    plt.plot(avg_losses[2], label='G_A')\n",
    "    plt.plot(avg_losses[3], label='G_B')\n",
    "    plt.plot(avg_losses[4], label='cycle_A')\n",
    "    plt.plot(avg_losses[5], label='cycle_B')\n",
    "    plt.legend()\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "        save_fn = save_dir + 'Loss_values_epoch_{:d}'.format(num_epochs) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_train_result(real_image, gen_image, recon_image, epoch, save=False, save_dir='results/', show=False, fig_size=(5, 5)):\n",
    "    \"\"\"\n",
    "    Plots a grid of real images, generated images, and reconstructed images produced by a GAN model at a given epoch.\n",
    "\n",
    "    Args:\n",
    "        real_image (torch.Tensor): A tensor of real images.\n",
    "        gen_image (torch.Tensor): A tensor of generated images.\n",
    "        recon_image (torch.Tensor): A tensor of reconstructed images.\n",
    "        epoch (int): The epoch number.\n",
    "        save (bool, optional): If True, saves the plot to a file. Defaults to False.\n",
    "        save_dir (str, optional): The directory where the plot should be saved. Defaults to 'results/'.\n",
    "        show (bool, optional): If True, displays the plot. Defaults to False.\n",
    "        fig_size (tuple, optional): The size of the figure. Defaults to (5, 5).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=fig_size)\n",
    "\n",
    "    imgs = [to_np(real_image[0]), to_np(gen_image[0]), to_np(recon_image[0]),\n",
    "            to_np(real_image[1]), to_np(gen_image[1]), to_np(recon_image[1])]\n",
    "    for ax, img in zip(axes.flatten(), imgs):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box')\n",
    "        # Scale to 0-255\n",
    "        img = img.squeeze()\n",
    "        img = (((img - img.min()) * 255) / (img.max() - img.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "        ax.imshow(img, cmap=None, aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    title = 'Epoch {0}'.format(epoch + 1)\n",
    "    fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "\n",
    "        save_fn = save_dir + 'Result_epoch_{:d}'.format(epoch+1) + '.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_test_result(real_image, gen_image, recon_image, index, save=False, save_dir='results/', show=False):\n",
    "    \"\"\"\n",
    "    Plots the real image, generated image, and reconstructed image for a single test sample.\n",
    "\n",
    "    Args:\n",
    "    - real_image: torch.Tensor of shape (batch_size, channels, height, width) representing the real image.\n",
    "    - gen_image: torch.Tensor of shape (batch_size, channels, height, width) representing the generated image.\n",
    "    - recon_image: torch.Tensor of shape (batch_size, channels, height, width) representing the reconstructed image.\n",
    "    - index: int representing the index of the test sample.\n",
    "    - save: bool flag indicating whether to save the plot to a file.\n",
    "    - save_dir: str representing the directory where the plot will be saved.\n",
    "    - show: bool flag indicating whether to display the plot.\n",
    "    \n",
    "    Returns: None.\n",
    "    \"\"\"\n",
    "    fig_size = (real_image.size(2) * 3 / 100, real_image.size(3) / 100)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=fig_size)\n",
    "\n",
    "    imgs = [to_np(real_image), to_np(gen_image), to_np(recon_image)]\n",
    "    for ax, img in zip(axes.flatten(), imgs):\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box')\n",
    "        # Scale to 0-255\n",
    "        img = img.squeeze()\n",
    "        img = (((img - img.min()) * 255) / (img.max() - img.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "        ax.imshow(img, cmap=None, aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "\n",
    "        save_fn = save_dir + 'Test_result_{:d}'.format(index + 1) + '.png'\n",
    "        fig.subplots_adjust(bottom=0)\n",
    "        fig.subplots_adjust(top=1)\n",
    "        fig.subplots_adjust(right=1)\n",
    "        fig.subplots_adjust(left=0)\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "# Make gif\n",
    "def make_gif(dataset, num_epochs, save_dir='results/', source_dir='results/'):\n",
    "    \"\"\"\n",
    "    Create a GIF by combining all the image plots saved during the training of a CycleGAN model.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Name of the dataset.\n",
    "        num_epochs (int): Number of epochs for which the image plots have been saved.\n",
    "        save_dir (str, optional): Directory to save the generated GIF. Defaults to 'results/'.\n",
    "        source_dir (str, optional): Directory where the image plots are saved. Defaults to 'results/'.\n",
    "    \"\"\"\n",
    "    gen_image_plots = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # plot for generating gif\n",
    "        save_fn = source_dir + 'Result_epoch_{:d}'.format(epoch + 1) + '.png'\n",
    "        gen_image_plots.append(imageio.imread(save_fn))\n",
    "\n",
    "    imageio.mimsave(save_dir + dataset + '_CycleGAN_epochs_{:d}'.format(num_epochs) + '.gif', gen_image_plots, fps=5)\n",
    "\n",
    "\n",
    "class ImagePool():\n",
    "    \"\"\"\n",
    "    Class for implementing an image pool for CycleGAN training.\n",
    "\n",
    "    Args:\n",
    "        pool_size (int): The maximum number of images to store in the pool.\n",
    "\n",
    "    Attributes:\n",
    "        pool_size (int): The maximum number of images to store in the pool.\n",
    "        num_imgs (int): The current number of images in the pool.\n",
    "        images (list): A list of images currently in the pool.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        \"\"\"\n",
    "        Query the image pool to retrieve a set of images.\n",
    "\n",
    "        If the pool is not full, the input images are added to the pool and returned\n",
    "        without modification. Otherwise, each input image is either added to the pool\n",
    "        with a probability of 0.5, or a random image from the pool is returned instead.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): A tensor of input images to retrieve from the pool.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of output images, either the original input images or\n",
    "            images retrieved from the pool.\n",
    "        \"\"\"\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images.data:\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size-1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGj40uYGfaz5"
   },
   "source": [
    "Set up the loss functions to be used during training and initializes some lists to store the average losses during training. Also initialize image pools to store generated images for use in training the generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIa7VMoMcQ2d"
   },
   "outputs": [],
   "source": [
    "MSE_Loss = torch.nn.MSELoss().cuda()\n",
    "L1_Loss = torch.nn.L1Loss().cuda()\n",
    "\n",
    "# # Training GAN\n",
    "D_A_avg_losses = []\n",
    "D_B_avg_losses = []\n",
    "G_A_avg_losses = []\n",
    "G_B_avg_losses = []\n",
    "cycle_A_avg_losses = []\n",
    "cycle_B_avg_losses = []\n",
    "\n",
    "# Generated image pool\n",
    "num_pool = 50\n",
    "fake_A_pool = ImagePool(num_pool)\n",
    "fake_B_pool = ImagePool(num_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgsZ8dOvfzvl"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3NAP_2TceWp",
    "outputId": "a8c4c4cd-a6a5-4124-b102-65717f6aedde"
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "for epoch in range(params.num_epochs):\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    G_A_losses = []\n",
    "    G_B_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "    \n",
    "    # Learing rate decay\n",
    "    if(epoch + 1) > params.decay_epoch:\n",
    "        D_A_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        D_B_optimizer.param_groups[0]['lr'] -= params.lrD / (params.num_epochs - params.decay_epoch)\n",
    "        G_optimizer.param_groups[0]['lr'] -= params.lrG / (params.num_epochs - params.decay_epoch)\n",
    "        \n",
    "    \n",
    "    # training\n",
    "    for i, (real_A, real_B) in enumerate(zip(train_data_loader_A, train_data_loader_B)):\n",
    "        \n",
    "        # input image data\n",
    "        real_A = Variable(real_A.cuda())\n",
    "        real_B = Variable(real_B.cuda())\n",
    "        \n",
    "        # -------------------------- train generator G --------------------------\n",
    "        # A --> B\n",
    "        fake_B = G_A(real_A)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        G_A_loss = MSE_Loss(D_B_fake_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # forward cycle loss\n",
    "        recon_A = G_B(fake_B)\n",
    "        cycle_A_loss = L1_Loss(recon_A, real_A) * params.lambdaA\n",
    "        \n",
    "        # B --> A\n",
    "        fake_A = G_B(real_B)\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        G_B_loss = MSE_Loss(D_A_fake_decision, Variable(torch.ones(D_A_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # backward cycle loss\n",
    "        recon_B = G_A(fake_A)\n",
    "        cycle_B_loss = L1_Loss(recon_B, real_B) * params.lambdaB\n",
    "        \n",
    "        # Back propagation\n",
    "        G_loss = G_A_loss + G_B_loss + cycle_A_loss + cycle_B_loss\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        \n",
    "        # -------------------------- train discriminator D_A --------------------------\n",
    "        D_A_real_decision = D_A(real_A)\n",
    "        D_A_real_loss = MSE_Loss(D_A_real_decision, Variable(torch.ones(D_A_real_decision.size()).cuda()))\n",
    "        \n",
    "        fake_A = fake_A_pool.query(fake_A)\n",
    "        \n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        D_A_fake_loss = MSE_Loss(D_A_fake_decision, Variable(torch.zeros(D_A_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # Back propagation\n",
    "        D_A_loss = (D_A_real_loss + D_A_fake_loss) * 0.5\n",
    "        D_A_optimizer.zero_grad()\n",
    "        D_A_loss.backward()\n",
    "        D_A_optimizer.step()\n",
    "        \n",
    "        # -------------------------- train discriminator D_B --------------------------\n",
    "        D_B_real_decision = D_B(real_B)\n",
    "        D_B_real_loss = MSE_Loss(D_B_real_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "        \n",
    "        fake_B = fake_B_pool.query(fake_B)\n",
    "        \n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        D_B_fake_loss = MSE_Loss(D_B_fake_decision, Variable(torch.zeros(D_B_fake_decision.size()).cuda()))\n",
    "        \n",
    "        # Back propagation\n",
    "        D_B_loss = (D_B_real_loss + D_B_fake_loss) * 0.5\n",
    "        D_B_optimizer.zero_grad()\n",
    "        D_B_loss.backward()\n",
    "        D_B_optimizer.step()\n",
    "        \n",
    "        # ------------------------ Print -----------------------------\n",
    "        # loss values\n",
    "        D_A_losses.append(D_A_loss.data)\n",
    "        D_B_losses.append(D_B_loss.data)\n",
    "        G_A_losses.append(G_A_loss.data)\n",
    "        G_B_losses.append(G_B_loss.data)\n",
    "        cycle_A_losses.append(cycle_A_loss.data)\n",
    "        cycle_B_losses.append(cycle_B_loss.data)\n",
    "\n",
    "        if i%10 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], D_A_loss: %.4f, D_B_loss: %.4f, G_A_loss: %.4f, G_B_loss: %.4f'\n",
    "                  % (epoch+1, params.num_epochs, i+1, len(train_data_loader_A), D_A_loss.data, D_B_loss.data, G_A_loss.data, G_B_loss.data))\n",
    "        step += 1\n",
    "        \n",
    "    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\n",
    "    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\n",
    "    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\n",
    "    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\n",
    "    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n",
    "    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_A_avg_losses.append(D_A_avg_loss)\n",
    "    D_B_avg_losses.append(D_B_avg_loss)\n",
    "    G_A_avg_losses.append(G_A_avg_loss)\n",
    "    G_B_avg_losses.append(G_B_avg_loss)\n",
    "    cycle_A_avg_losses.append(cycle_A_avg_loss)\n",
    "    cycle_B_avg_losses.append(cycle_B_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "    test_real_A = Variable(test_real_A_data.cuda())\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = Variable(test_real_B_data.cuda())\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "\n",
    "    plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B],\n",
    "                            epoch, save=True, save_dir=save_dir)\n",
    "\n",
    "    # log the images\n",
    "    result_AtoB = np.concatenate((to_np(test_real_A), to_np(test_fake_B), to_np(test_recon_A)), axis=3)\n",
    "    result_BtoA = np.concatenate((to_np(test_real_B), to_np(test_fake_A), to_np(test_recon_B)), axis=3)\n",
    "\n",
    "    info = { 'result_AtoB': result_AtoB.transpose(0, 2, 3, 1),  # convert to BxHxWxC\n",
    "             'result_BtoA': result_BtoA.transpose(0, 2, 3, 1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6EhgSCvhN42"
   },
   "source": [
    "Plotting and saving the average losses over the epochs and also creating a GIF of the generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HADHCe9GxxhJ",
    "outputId": "73f202bd-facf-4cda-d8ef-b2cee45d9e9b"
   },
   "outputs": [],
   "source": [
    "# Plot average losses\n",
    "avg_losses = []\n",
    "avg_losses.append(D_A_avg_losses)\n",
    "avg_losses.append(D_B_avg_losses)\n",
    "avg_losses.append(G_A_avg_losses)\n",
    "avg_losses.append(G_B_avg_losses)\n",
    "avg_losses.append(cycle_A_avg_losses)\n",
    "avg_losses.append(cycle_B_avg_losses)\n",
    "plot_loss(avg_losses, params.num_epochs, save=True, save_dir=plot_gif_dir)\n",
    "\n",
    "# Make gif\n",
    "make_gif(params.dataset, params.num_epochs, save_dir=plot_gif_dir, source_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K06XHQ5Kh8bc"
   },
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzo0Y44ZhRGK"
   },
   "source": [
    "Generating test results for both the A-to-B and B-to-A directions. For each direction, we loop through the test data loader and generate fake images using the corresponding generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_u_Z-oRhSEp",
    "outputId": "e2aab3a2-4bf1-465f-d310-a0255f16c9cf"
   },
   "outputs": [],
   "source": [
    "for i, real_A in enumerate(test_data_loader_A):\n",
    "    # input image data\n",
    "    real_A = Variable(real_A.cuda())\n",
    "    \n",
    "    # A --> B --> A\n",
    "    fake_B = G_A(real_A)\n",
    "    recon_A = G_B(fake_B)\n",
    "    \n",
    "    # Show result for test data\n",
    "    plot_test_result(real_A, fake_B, recon_A, i, save=True, save_dir=test_res_dir + 'AtoB/')\n",
    "\n",
    "    print('%d images are generated.' % (i + 1))\n",
    "\n",
    "for i, real_B in enumerate(test_data_loader_B):\n",
    "\n",
    "    # input image data\n",
    "    real_B = Variable(real_B.cuda())\n",
    "\n",
    "    # B -> A -> B\n",
    "    fake_A = G_B(real_B)\n",
    "    recon_B = G_A(fake_A)\n",
    "\n",
    "    # Show result for test data\n",
    "    plot_test_result(real_B, fake_A, recon_B, i, save=True, save_dir=test_res_dir + 'BtoA/')\n",
    "\n",
    "    print('%d images are generated.' % (i + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOOrObSyhV4v"
   },
   "source": [
    "# Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ktkzyOrGFLPg",
    "outputId": "4a559bb2-a70b-410e-afd6-3d99c7ea9e5a"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/results.zip /content/results\n",
    "!zip -r /content/test_results.zip /content/test_results\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"/content/results.zip\")\n",
    "files.download(\"/content/test_results.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
